{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-palnitkar/ODSC-25/blob/main/notebooks/01_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce7707e",
      "metadata": {
        "id": "fce7707e"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "This notebook is part of a tutorial on \"Practical Bayesian Modeling with PyMC\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c2f26e",
      "metadata": {
        "id": "48c2f26e"
      },
      "source": [
        "[Click here to run this notebook on Colab](https://colab.research.google.com/github/AllenDowney/SurveyDataPyMC/blob/main/notebooks/01_logistic_regression.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce89fc0",
      "metadata": {
        "id": "fce89fc0"
      },
      "outputs": [],
      "source": [
        "# Get utils.py\n",
        "\n",
        "from os.path import basename, exists\n",
        "\n",
        "\n",
        "def download(url):\n",
        "    filename = basename(url)\n",
        "    if not exists(filename):\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "        local, _ = urlretrieve(url, filename)\n",
        "        print(\"Downloaded \" + local)\n",
        "\n",
        "\n",
        "download(\"https://github.com/AllenDowney/SurveyDataPyMC/raw/main/notebooks/utils.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24d9f87",
      "metadata": {
        "id": "f24d9f87"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "\n",
        "from utils import value_counts, decorate, load_idata_or_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860f08d5",
      "metadata": {
        "id": "860f08d5"
      },
      "outputs": [],
      "source": [
        "# Make the figures smaller to save some screen real estate\n",
        "plt.rcParams[\"figure.dpi\"] = 75\n",
        "plt.rcParams[\"figure.figsize\"] = [6, 3.5]\n",
        "plt.rcParams[\"axes.titlelocation\"] = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d12ba9",
      "metadata": {
        "id": "f6d12ba9"
      },
      "source": [
        "## Data\n",
        "\n",
        "The dataset we'll use is an extract from the General Social Survey.\n",
        "The following cell downloads it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad453fb",
      "metadata": {
        "id": "0ad453fb"
      },
      "outputs": [],
      "source": [
        "# This dataset is prepared in GssExtract/notebooks/02_make_extract-2022_4.ipynb\n",
        "# It has been resampled to correct for stratified sampling\n",
        "\n",
        "DATA_PATH = \"https://github.com/AllenDowney/GssExtract/raw/main/data/interim/\"\n",
        "filename = \"gss_extract_2022_4.hdf\"\n",
        "download(DATA_PATH + filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2794fd",
      "metadata": {
        "id": "4e2794fd"
      },
      "source": [
        "The file is in HDF format, which we can read using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91bdaed",
      "metadata": {
        "id": "b91bdaed"
      },
      "outputs": [],
      "source": [
        "gss = pd.read_hdf(filename, \"gss\")\n",
        "gss.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250d175e",
      "metadata": {
        "id": "250d175e"
      },
      "source": [
        "The dataset includes one row for each respondent and one column for each variable.\n",
        "\n",
        "To demonstrate logistic regression, we'll use responses to [this question](https://gssdataexplorer.norc.org/variables/441/vshow)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10535f31",
      "metadata": {
        "id": "10535f31"
      },
      "source": [
        "> Generally speaking, would you say that most people can be trusted or that you can't be too careful in dealing with people?\n",
        "\n",
        "```\n",
        "1\tMost people can be trusted\n",
        "2\tCan't be too careful\n",
        "3\tDepends\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b924e423",
      "metadata": {
        "id": "b924e423"
      },
      "source": [
        "Here are the counts of the responses -- there are a large number of NaN values because not every respondent was asked the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d77833",
      "metadata": {
        "id": "13d77833"
      },
      "outputs": [],
      "source": [
        "value_counts(gss[\"trust\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3515c58c",
      "metadata": {
        "id": "3515c58c"
      },
      "source": [
        "Although there are three possible responses, we'll treat this as a binary variable.\n",
        "So we'll recode the data so `1` means \"most people can be trusted\" and `0` means either of the other responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb2d5e8",
      "metadata": {
        "id": "dbb2d5e8"
      },
      "outputs": [],
      "source": [
        "gss[\"y\"] = gss[\"trust\"].replace([1, 2, 3], [1, 0, 0])\n",
        "value_counts(gss[\"y\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79cf376a",
      "metadata": {
        "id": "79cf376a"
      },
      "source": [
        "Here's what the percentage of affirmative responses looks like over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca78bde",
      "metadata": {
        "id": "6ca78bde"
      },
      "outputs": [],
      "source": [
        "time_series = gss.groupby(\"year\")[\"y\"].mean() * 100\n",
        "time_series.plot(style=\"o\")\n",
        "decorate(ylabel=\"percent\", title=\"Can people be trusted?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2506ffe4",
      "metadata": {
        "id": "2506ffe4"
      },
      "source": [
        "Sadly, levels of trust have been declining in the US for decades."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05bc2da",
      "metadata": {
        "id": "a05bc2da"
      },
      "source": [
        "## Who is most trusting?\n",
        "\n",
        "Who do you think is most trusting, Democrats, Republicans, or independents?\n",
        "To find out, we'll use another [GSS variable](https://gssdataexplorer.norc.org/variables/141/vshow), which contains responses to this question:\n",
        "\n",
        "> Generally speaking, do you usually think of yourself as a Republican, Democrat, Independent, or what?\n",
        "\n",
        "```\n",
        "0\tStrong democrat\n",
        "1\tNot very strong democrat\n",
        "2\tIndependent, close to democrat\n",
        "3\tIndependent (neither, no response)\n",
        "4\tIndependent, close to republican\n",
        "5\tNot very strong republican\n",
        "6\tStrong republican\n",
        "7\tOther party\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12935ee",
      "metadata": {
        "id": "e12935ee"
      },
      "source": [
        "To simplify the analysis, we'll consider just three groups, Democrats (strong or not), Independent (leaning either way) and Republican (strong or not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c65cd07",
      "metadata": {
        "id": "8c65cd07"
      },
      "outputs": [],
      "source": [
        "party_map = {\n",
        "    0: 0,\n",
        "    1: 0,\n",
        "    2: 1,\n",
        "    3: 1,\n",
        "    4: 1,\n",
        "    5: 2,\n",
        "    6: 2,\n",
        "    7: np.nan,\n",
        "}\n",
        "\n",
        "gss[\"partyid3\"] = gss[\"partyid\"].replace(party_map)\n",
        "value_counts(gss[\"partyid3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b6f15e",
      "metadata": {
        "id": "c8b6f15e"
      },
      "source": [
        "Well use this dictionary to map from codes to names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ea06a2",
      "metadata": {
        "id": "07ea06a2"
      },
      "outputs": [],
      "source": [
        "party_names = {\n",
        "    0: \"Democrat\",\n",
        "    1: \"Independent\",\n",
        "    2: \"Republican\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3459c29a",
      "metadata": {
        "id": "3459c29a"
      },
      "source": [
        "The following table shows how the percentages in each group have changed over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f1c7df",
      "metadata": {
        "id": "f5f1c7df"
      },
      "outputs": [],
      "source": [
        "table = (\n",
        "    gss.pivot_table(index=\"year\", columns=\"partyid3\", values=\"y\", aggfunc=\"mean\")\n",
        ").rename(columns=party_names) * 100\n",
        "\n",
        "table.iloc[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a64e2f",
      "metadata": {
        "id": "33a64e2f"
      },
      "source": [
        "Here's what the columns look like as time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8751b5",
      "metadata": {
        "id": "ca8751b5"
      },
      "outputs": [],
      "source": [
        "colors = [\"C0\", \"C4\", \"C3\"]\n",
        "table.plot(color=colors)\n",
        "decorate(ylabel=\"percent\", title=\"Percent saying people can be trusted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c06d2d",
      "metadata": {
        "id": "19c06d2d"
      },
      "source": [
        "It looks like Republicans were the most trusting group, historically, but that might be changing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebc180b",
      "metadata": {
        "id": "bebc180b"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "We'll use logistic regression to model changes in these responses over time and differences between groups.\n",
        "\n",
        "The fundamental idea of logistic regression is that each observation unit -- like a survey respondent -- has some latent propensity to choose one of two options, and we assume:\n",
        "\n",
        "* The latent propensities, `z[i]`, are a linear function of explanatory variables.\n",
        "\n",
        "* The probability a respondent chooses a particular option is `expit(z[i])`.\n",
        "\n",
        "Where `expit` is the function that maps from log-odds to probability (defined in `scipy.special`, also available from PyMC as `pm.math.sigmoid`).\n",
        "\n",
        "As a first example, we'll make a model with `year` as an explanatory variable, so we'll assume\n",
        "\n",
        "```\n",
        "z = alpha + beta * year\n",
        "```\n",
        "\n",
        "In a minute we'll see how to represent this model in PyMC, but first let's prepare the data.\n",
        "We'll select the rows with valid data for the response variable and `year`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8e6e93",
      "metadata": {
        "id": "3c8e6e93"
      },
      "outputs": [],
      "source": [
        "data = gss.dropna(subset=[\"y\", \"year\"])\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ab91000",
      "metadata": {
        "id": "2ab91000"
      },
      "source": [
        "And we'll extract the values as NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cb2fca",
      "metadata": {
        "id": "b1cb2fca"
      },
      "outputs": [],
      "source": [
        "y = data[\"y\"].to_numpy()\n",
        "year = data[\"year\"].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc91d9d",
      "metadata": {
        "id": "9dc91d9d"
      },
      "source": [
        "We'll shift `year` so it's centered at its mean (and we'll see why later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9e87a2",
      "metadata": {
        "id": "ca9e87a2"
      },
      "outputs": [],
      "source": [
        "year_shift = data[\"year\"].mean()\n",
        "year = year - year_shift"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be70c97",
      "metadata": {
        "id": "7be70c97"
      },
      "source": [
        "Now here's the key idea of PyMC (and MCMC in general): if you can describe the data-generating process, the sampler can generate a sample from the posterior distribution of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43eb6e01",
      "metadata": {
        "id": "43eb6e01"
      },
      "outputs": [],
      "source": [
        "# Fill this in\n",
        "\n",
        "with pm.Model() as logistic_model:\n",
        "\n",
        "    # TODO: Choose priors for alpha and beta\n",
        "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n",
        "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
        "\n",
        "    # TODO: Define the linear predictor\n",
        "    z = alpha + beta * year\n",
        "\n",
        "    # TODO: Apply the sigmoid function to get probabilities\n",
        "    p = pm.math.sigmoid(z)\n",
        "\n",
        "    # TODO: Define the likelihood using a Bernoulli distribution\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f681bea",
      "metadata": {
        "id": "5f681bea"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "\n",
        "with pm.Model() as logistic_model:\n",
        "\n",
        "    # Priors for intercept and slope\n",
        "    alpha = pm.Normal(\"alpha\", 0, 1)\n",
        "    beta = pm.Normal(\"beta\", 0, 1)\n",
        "\n",
        "    # Linear predictor (log-odds)\n",
        "    z = alpha + beta * year\n",
        "\n",
        "    # The inverse logit function is called sigma\n",
        "    p = pm.math.sigmoid(z)\n",
        "\n",
        "    # Bernoulli likelihood with logit link\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a59f2a",
      "metadata": {
        "id": "c2a59f2a"
      },
      "source": [
        "Objects created in the context manager are registered as elements of the model.\n",
        "Here's a graphical representation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29062a7f",
      "metadata": {
        "id": "29062a7f"
      },
      "outputs": [],
      "source": [
        "pm.model_to_graphviz(logistic_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce41d872",
      "metadata": {
        "id": "ce41d872"
      },
      "source": [
        "## The posterior distribution\n",
        "\n",
        "The function that samples from the posterior distribution is called `sample`.\n",
        "\n",
        "```\n",
        "with logistic_model:\n",
        "    idata = pm.sample(draws=500, tune=500)\n",
        "```\n",
        "\n",
        "`draw` indicates the number of samples we want from each chain.\n",
        "`tune` is the number of samples used to tune the chains before we start saving values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d27bd8",
      "metadata": {
        "id": "e4d27bd8"
      },
      "source": [
        "For the workshop, we'll use the following function, which loads the results if they are already saved, or runs the sampler otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb6a14e",
      "metadata": {
        "id": "4bb6a14e"
      },
      "outputs": [],
      "source": [
        "filename = \"logistic_model_idata.nc\"\n",
        "idata = load_idata_or_sample(logistic_model, filename, draws=500, tune=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b8c8c0",
      "metadata": {
        "id": "09b8c8c0"
      },
      "source": [
        "The result is an [ArViz `InferenceData` object](https://python.arviz.org/en/stable/api/inference_data.html), which contains several groups of data stored as [xarray `DataSet` objects](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b56d08e",
      "metadata": {
        "id": "8b56d08e"
      },
      "outputs": [],
      "source": [
        "idata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a413f5d",
      "metadata": {
        "id": "8a413f5d"
      },
      "source": [
        "ArViz provides a variety of functions for processing and visualizing the results.\n",
        "In this example, the primary thing we're interested in is the posterior distributions of the coefficients `alpha` and `beta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2731f31a",
      "metadata": {
        "id": "2731f31a"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(idata)\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38600bd5",
      "metadata": {
        "id": "38600bd5"
      },
      "source": [
        "## Comparing Prior with Posterior\n",
        "\n",
        "To see what we learned from the data, we can compare the prior and posterior distributions of the coefficents.\n",
        "`sample_prior_predictive` runs the model forward to generate samples from the prior distribution (and the prior predictive, which we'll talk about later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbb064e",
      "metadata": {
        "id": "8dbb064e"
      },
      "outputs": [],
      "source": [
        "with logistic_model:\n",
        "    idata_prior = pm.sample_prior_predictive(draws=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2137685a",
      "metadata": {
        "id": "2137685a"
      },
      "source": [
        "The result is another `InferenceData` object.\n",
        "It will be convenient to put all of the samples in one object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4043aeda",
      "metadata": {
        "id": "4043aeda"
      },
      "outputs": [],
      "source": [
        "idata.extend(idata_prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4522662",
      "metadata": {
        "id": "f4522662"
      },
      "source": [
        "Now we can compare the prior and posterior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca849b4a",
      "metadata": {
        "id": "ca849b4a"
      },
      "outputs": [],
      "source": [
        "az.plot_dist_comparison(idata, var_names=[\"alpha\"])\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a7e48d",
      "metadata": {
        "id": "e2a7e48d"
      },
      "outputs": [],
      "source": [
        "az.plot_dist_comparison(idata, var_names=[\"beta\"])\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500bb996",
      "metadata": {
        "id": "500bb996"
      },
      "source": [
        "Looking at the posterior distributions of `alpha` and `beta`, we can see that they fall comfortably within the prior distributions of these parameters, which means that the priors didn't have much effect on the results.\n",
        "\n",
        "As an experiment, try increasing or decreasing `sigma_prior` and see what effect it has on the posterior distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1af0d8",
      "metadata": {
        "id": "0c1af0d8"
      },
      "source": [
        "## Sampling diagnostics\n",
        "\n",
        "After sampling, we want to check that the process worked well — meaning:\n",
        "\n",
        "* All chains have effectively explored the posterior distribution, and\n",
        "\n",
        "* Successive draws within each chain are only weakly correlated (each draw should provide mostly new information).\n",
        "\n",
        "We can check these properties visually using `plot_trace`:\n",
        "\n",
        "*    On the left, the distribution from each chain should look similar — this is evidence that the chains all converged to the same region of the posterior.\n",
        "\n",
        "*    On the right, the sequence of draws within each chain should look like uncorrelated noise (sometimes called \"hairy caterpillars\"). It shouldn't look like a random walk and there shouldn't be flat spots, which would suggest the chain got stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40880262",
      "metadata": {
        "id": "40880262"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(idata, var_names=[\"alpha\", \"beta\"])\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59ff135",
      "metadata": {
        "id": "d59ff135"
      },
      "source": [
        "To check these properties numerically, we can look at two key summary statistics: ESS and r-hat.\n",
        "\n",
        "*    r-hat quantifies the difference between chains. If all chains converged to the same posterior distribution, r-hat should be close to 1. If the chains are exploring different regions, r-hat will be larger than 1, indicating failure to converge.\n",
        "\n",
        "*    Effective Sample Size (ESS) tells us how much independent information the draws actually contribute. If successive values within a chain are highly correlated, the chain isn’t exploring the posterior efficiently, and ESS will be smaller than the total number of draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab460206",
      "metadata": {
        "id": "ab460206"
      },
      "outputs": [],
      "source": [
        "az.summary(idata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bc8cf7",
      "metadata": {
        "id": "d0bc8cf7"
      },
      "source": [
        "## Generating predictions\n",
        "\n",
        "There are two ways to generate predictions:\n",
        "\n",
        "* We can use the results from PyMC to compute our own predictions.\n",
        "\n",
        "* We can use the PyMC model.\n",
        "\n",
        "With this example, we'll demonstrate the first way.\n",
        "\n",
        "First, we'll extract the samples of the coefficients from `idata` and convert them to NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a1f9b4",
      "metadata": {
        "id": "c2a1f9b4"
      },
      "outputs": [],
      "source": [
        "# Collect posterior draws of alpha and beta\n",
        "samples = az.extract(idata, var_names=[\"alpha\", \"beta\"], num_samples=100)\n",
        "\n",
        "alphas = samples[\"alpha\"].to_numpy()\n",
        "alphas.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d272e139",
      "metadata": {
        "id": "d272e139"
      },
      "outputs": [],
      "source": [
        "betas = samples[\"beta\"].to_numpy()\n",
        "betas.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a478af",
      "metadata": {
        "id": "e9a478af"
      },
      "source": [
        "Here's the range of years we'll predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c403beca",
      "metadata": {
        "id": "c403beca"
      },
      "outputs": [],
      "source": [
        "year_range = np.arange(1970, 2031)\n",
        "year_centered = year_range - year_shift"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4071586c",
      "metadata": {
        "id": "4071586c"
      },
      "source": [
        "To generate predictions, we pretty much reimplement the model in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6cbc192",
      "metadata": {
        "id": "b6cbc192"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit\n",
        "\n",
        "for alpha, beta in zip(alphas, betas):\n",
        "    zs = alpha + beta * year_centered\n",
        "    probs = expit(zs) * 100\n",
        "    plt.plot(year_range, probs, color=\"C0\", alpha=0.02)\n",
        "\n",
        "time_series.plot(style=\"o\", label=\"observed\")\n",
        "decorate(ylabel=\"percent\", title=\"Percent saying people can be trusted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d13821",
      "metadata": {
        "id": "01d13821"
      },
      "source": [
        "It looks like the model fits the data well, and makes a plausible projection for the future."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05abf2a4",
      "metadata": {
        "id": "05abf2a4"
      },
      "source": [
        "## Centering the Data\n",
        "\n",
        "You might remember that we mean-centered the predictor, `years`.\n",
        "Now here's why: because we centered `years`, the sampled slopes and intercepts are uncorrelated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2659c2f",
      "metadata": {
        "id": "a2659c2f"
      },
      "outputs": [],
      "source": [
        "pm.plot_pair(idata, var_names=[\"alpha\", \"beta\"])\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbc7230",
      "metadata": {
        "id": "cdbc7230"
      },
      "source": [
        "As an experiment, try:\n",
        "\n",
        "* Set `year_shift=1970` and run the model again. You might get some warnings from the sampler, and `plot_pair` will show that the alphas and betas are correlated. The not-quite-centered predictor stretches the shape of the joint posterior distribution and makes it harder to sample efficiently.\n",
        "\n",
        "* Set `year_shift=0` and run the model again. With the predictor completely uncentered, the joint posterior distribution is so stretched that the sampler diverges frequently -- and basically fails.\n",
        "\n",
        "Without centering, the intercept represents the log-odds of the outcome when `year=0`, which is way outside the range of the data.\n",
        "This forces the intercept and slope to compensate for each other — if the slope increases, the intercept must decrease to hit the same observed points.\n",
        "\n",
        "As a general rule, it's a good idea to center continuous predictors in Bayesian regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc3812e",
      "metadata": {
        "id": "9cc3812e"
      },
      "source": [
        "## Categorical predictors\n",
        "\n",
        "Now let's add political party as a predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6258988e",
      "metadata": {
        "id": "6258988e"
      },
      "outputs": [],
      "source": [
        "value_counts(gss[\"partyid3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f84b7c",
      "metadata": {
        "id": "80f84b7c"
      },
      "source": [
        "To prepare the data, we'll select observations where all the variables in the model are valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633c1c80",
      "metadata": {
        "id": "633c1c80"
      },
      "outputs": [],
      "source": [
        "data = gss.dropna(subset=[\"y\", \"year\", \"partyid3\"])\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48456bf",
      "metadata": {
        "id": "d48456bf"
      },
      "source": [
        "And again we'll center the years and convert all data to NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d7c517",
      "metadata": {
        "id": "c8d7c517"
      },
      "outputs": [],
      "source": [
        "y = data[\"y\"].to_numpy()\n",
        "\n",
        "year = data[\"year\"].to_numpy()\n",
        "year_shift = data[\"year\"].mean()\n",
        "year = year - year_shift\n",
        "\n",
        "partyid3 = data[\"partyid3\"].astype(int).to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e6cef18",
      "metadata": {
        "id": "4e6cef18"
      },
      "source": [
        "This version of the model adds a separate intercept for each group, but still has a single coefficient for the age effect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed81157c",
      "metadata": {
        "id": "ed81157c"
      },
      "outputs": [],
      "source": [
        "# Modify this\n",
        "\n",
        "with pm.Model() as logistic_model2:\n",
        "\n",
        "    # Priors for intercept and slope\n",
        "    alpha = pm.Normal(\"alpha\", 0, 1)\n",
        "    beta = pm.Normal(\"beta\", 0, 1)\n",
        "\n",
        "    # Linear predictor (log-odds)\n",
        "    z = alpha + beta * year\n",
        "\n",
        "    # The inverse logit function is called sigma\n",
        "    p = pm.math.sigmoid(z)\n",
        "\n",
        "    # Bernoulli likelihood with logit link\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f2effe",
      "metadata": {
        "id": "34f2effe"
      },
      "outputs": [],
      "source": [
        "# Solution\n",
        "\n",
        "with pm.Model() as logistic_model2:\n",
        "\n",
        "    # Party-specific intercepts (one for each group)\n",
        "    alpha = pm.Normal(\"alpha\", 0, 1, shape=3)\n",
        "\n",
        "    # Shared slope for year (assuming effect of year is the same across parties)\n",
        "    beta = pm.Normal(\"beta\", 0, 1)\n",
        "\n",
        "    # Linear predictor (log-odds)\n",
        "    z = alpha[partyid3] + beta * year\n",
        "\n",
        "    # Compute and save the probabilities\n",
        "    p = pm.math.sigmoid(z)\n",
        "\n",
        "    # Bernoulli likelihood\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ede711",
      "metadata": {
        "id": "b9ede711"
      },
      "source": [
        "Here's the graph representation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe23b258",
      "metadata": {
        "id": "fe23b258"
      },
      "outputs": [],
      "source": [
        "pm.model_to_graphviz(logistic_model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c25b509",
      "metadata": {
        "id": "8c25b509"
      },
      "source": [
        "And here's how we run the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826222d4",
      "metadata": {
        "id": "826222d4"
      },
      "outputs": [],
      "source": [
        "filename = 'logistic_model2_idata.nc'\n",
        "idata2 = load_idata_or_sample(logistic_model2, filename, draws=500, tune=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be885ed",
      "metadata": {
        "id": "0be885ed"
      },
      "source": [
        "Here are the trace plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc186d5",
      "metadata": {
        "id": "4cc186d5"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(idata2, compact=False, var_names=[\"alpha\", \"beta\"])\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65654f1",
      "metadata": {
        "id": "b65654f1"
      },
      "source": [
        "And the diagnostic statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5afd4d",
      "metadata": {
        "id": "2f5afd4d"
      },
      "outputs": [],
      "source": [
        "pm.summary(idata2, var_names=[\"alpha\", \"beta\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2716b0bb",
      "metadata": {
        "id": "2716b0bb"
      },
      "source": [
        "Here are the posteriors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bca2430",
      "metadata": {
        "scrolled": true,
        "id": "7bca2430"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(idata2)\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43fe09a",
      "metadata": {
        "id": "e43fe09a"
      },
      "source": [
        "## Viewing the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e741bed1",
      "metadata": {
        "id": "e741bed1"
      },
      "outputs": [],
      "source": [
        "# Collect posterior draws of alpha and beta\n",
        "samples = az.extract(idata2, var_names=[\"alpha\", \"beta\"], num_samples=100)\n",
        "\n",
        "alphas = samples[\"alpha\"].to_numpy()\n",
        "alphas.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcfb0d1",
      "metadata": {
        "id": "2dcfb0d1"
      },
      "outputs": [],
      "source": [
        "betas = samples[\"beta\"].to_numpy()\n",
        "betas.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755606c3",
      "metadata": {
        "id": "755606c3"
      },
      "source": [
        "Here's the range of years we'll predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df058af",
      "metadata": {
        "id": "8df058af"
      },
      "outputs": [],
      "source": [
        "year_range = np.arange(1970, 2031)\n",
        "year_centered = year_range - year_shift"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7f3c54",
      "metadata": {
        "id": "ee7f3c54"
      },
      "source": [
        "Now we can plot the results for the three groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d2631cc",
      "metadata": {
        "id": "0d2631cc"
      },
      "outputs": [],
      "source": [
        "table.plot(color=colors)\n",
        "\n",
        "for i in range(3):\n",
        "    for alpha, beta in zip(alphas[i], betas):\n",
        "        logits = alpha + beta * year_centered\n",
        "        probs = expit(logits) * 100\n",
        "        plt.plot(year_range, probs, color=colors[i], alpha=0.02)\n",
        "\n",
        "decorate(ylabel=\"percent\", title=\"Percent saying people can be trusted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "528ec12b",
      "metadata": {
        "id": "528ec12b"
      },
      "source": [
        "The posterior distributions of `p` overlap for the Democrat and Independent groups, but the distribution for Republicans is notably different.\n",
        "\n",
        "Based on these results, we can say with some confidence that Republicans are more likely to say people can be trusted -- at least under the assumption that the changes over time are well modeled by the logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015a5c10",
      "metadata": {
        "id": "015a5c10"
      },
      "source": [
        "## Let the Model Do the Work\n",
        "\n",
        "It might seem silly that we have to implement the model twice, once in PyMC to generate the posterior distribution, and once in NumPy to interpret the results.\n",
        "\n",
        "It is possible -- and often desirable -- to make the PyMC model do the work, but for that we'll need two additional features:\n",
        "\n",
        "* `Data`, which makes the input data part of the model so we can modify it to make out-of-sample predictions, and\n",
        "\n",
        "* `Deterministic`, which saves the result of intermediate calculations as part of the `InferenceData`.\n",
        "\n",
        "Here's the augmented version of the previous model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c29ec21",
      "metadata": {
        "id": "0c29ec21"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as logistic_model3:\n",
        "\n",
        "    # add the predictors to the model as mutable Data\n",
        "    year_pt = pm.Data(\"year\", year)\n",
        "    partyid3_pt = pm.Data(\"partyid3\", partyid3)\n",
        "\n",
        "    # Party-specific intercepts (one for each group)\n",
        "    alpha = pm.Normal(\"alpha\", 0, 1, shape=3)\n",
        "\n",
        "    # Shared slope for year (assuming effect of year is the same across parties)\n",
        "    beta = pm.Normal(\"beta\", 0, 1)\n",
        "\n",
        "    # Linear predictor (log-odds)\n",
        "    z = alpha[partyid3_pt] + beta * year_pt\n",
        "\n",
        "    # Compute and save the probabilities\n",
        "    p = pm.Deterministic(\"p\", pm.math.sigmoid(z))\n",
        "\n",
        "    # Bernoulli likelihood\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8815bed",
      "metadata": {
        "id": "b8815bed"
      },
      "source": [
        "Here's the graph representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd6dcdb",
      "metadata": {
        "id": "edd6dcdb"
      },
      "outputs": [],
      "source": [
        "pm.model_to_graphviz(logistic_model3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad57572",
      "metadata": {
        "id": "5ad57572"
      },
      "source": [
        "And here's how we run the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2786de1d",
      "metadata": {
        "id": "2786de1d"
      },
      "outputs": [],
      "source": [
        "filename = 'logistic_model3_idata.nc'\n",
        "idata3 = load_idata_or_sample(logistic_model3, filename, draws=500, tune=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6ac023",
      "metadata": {
        "id": "0c6ac023"
      },
      "source": [
        "Everything marked as `Deterministic` gets saved in the `idata`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e677e8",
      "metadata": {
        "id": "01e677e8"
      },
      "outputs": [],
      "source": [
        "idata3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de123b1",
      "metadata": {
        "id": "4de123b1"
      },
      "source": [
        "Now to generate predictions, we can use the PyMC model directly.\n",
        "We'll use:\n",
        "\n",
        "* `set_data` to replace the data used during sampling with the values of `partyid3` and `year` we want to use for prediction, and\n",
        "\n",
        "* `compute_deterministics` to compute the values of `p`, conditioned on the posterior distributions of `alpha` and `beta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96bc203c",
      "metadata": {
        "id": "96bc203c"
      },
      "outputs": [],
      "source": [
        "probabilities = {}\n",
        "\n",
        "with logistic_model3:\n",
        "    for key, party_name in party_names.items():\n",
        "        # Assign covariates to compute the posterior distributions of\n",
        "        pm.set_data(\n",
        "            {\n",
        "                \"partyid3\": np.tile(key, len(year_centered)),\n",
        "                \"year\": year_centered,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Compute the posterior predictive\n",
        "        idata = pm.compute_deterministics(idata3['posterior'])\n",
        "\n",
        "        probabilities[party_name] = az.extract(idata, num_samples=100)[\"p\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1aa599a",
      "metadata": {
        "id": "c1aa599a"
      },
      "source": [
        "Here's what the results look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c861e4",
      "metadata": {
        "id": "a9c861e4"
      },
      "outputs": [],
      "source": [
        "# Plot the original data\n",
        "table.plot(color=colors)\n",
        "\n",
        "# Plot posterior draws for each party affiliation\n",
        "for key, party_name in party_names.items():\n",
        "    ps = probabilities[party_name].to_numpy() * 100\n",
        "    plt.plot(year_range, ps, color=colors[key], alpha=0.02)\n",
        "\n",
        "decorate(ylabel=\"percent\", title=\"Percent saying people can be trusted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdade23",
      "metadata": {
        "id": "ccdade23"
      },
      "source": [
        "Using the PyMC model to generate predictions avoids implementing the model twice, and guarantees that the model and the predictions are consistent.\n",
        "The drawbacks are:\n",
        "\n",
        "* Adding `Data` and `Deterministic` can make the model code harder to read.\n",
        "\n",
        "* Saving the `Data` and `Deterministic` variables makes the `InferenceData` larger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70d1249",
      "metadata": {
        "id": "d70d1249"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}